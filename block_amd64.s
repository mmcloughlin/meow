// Code generated by go run make_block.go. DO NOT EDIT.

// +build !noasm

#include "textflag.h"

TEXT ·checksum128(SB),0,$32-56
#define SEED R8
	MOVQ     seed+0(FP), SEED
#define DST_PTR DI
	MOVQ     dst_ptr+8(FP), DST_PTR
#define SRC_PTR SI
	MOVQ     src_ptr+32(FP), SRC_PTR
#define SRC_LEN AX
	MOVQ     src_len+40(FP), SRC_LEN

	// Backup total input length.
#define TOTAL_LEN R9
	MOVQ     SRC_LEN, TOTAL_LEN

	// Load zero "IV".
	PXOR     X0, X0
	PXOR     X1, X1
	PXOR     X2, X2
	PXOR     X3, X3
	PXOR     X4, X4
	PXOR     X5, X5
	PXOR     X6, X6
	PXOR     X7, X7
	PXOR     X8, X8
	PXOR     X9, X9
	PXOR     X10, X10
	PXOR     X11, X11
	PXOR     X12, X12
	PXOR     X13, X13
	PXOR     X14, X14
	PXOR     X15, X15

	// Handle full 256-byte blocks.

loop:
	CMPQ     SRC_LEN, $256
	JB       sub256

	// Hash block.
	VAESDEC  0(SRC_PTR), X0, X0
	VAESDEC  16(SRC_PTR), X1, X1
	VAESDEC  32(SRC_PTR), X2, X2
	VAESDEC  48(SRC_PTR), X3, X3
	VAESDEC  64(SRC_PTR), X4, X4
	VAESDEC  80(SRC_PTR), X5, X5
	VAESDEC  96(SRC_PTR), X6, X6
	VAESDEC  112(SRC_PTR), X7, X7
	VAESDEC  128(SRC_PTR), X8, X8
	VAESDEC  144(SRC_PTR), X9, X9
	VAESDEC  160(SRC_PTR), X10, X10
	VAESDEC  176(SRC_PTR), X11, X11
	VAESDEC  192(SRC_PTR), X12, X12
	VAESDEC  208(SRC_PTR), X13, X13
	VAESDEC  224(SRC_PTR), X14, X14
	VAESDEC  240(SRC_PTR), X15, X15

	// Update source pointer.
	ADDQ     $256, SRC_PTR
	SUBQ     $256, SRC_LEN
	JMP      loop

	// Handle final sub 256-byte block.

sub256:

	// Allocate general purpose registers.
#define MIX0 R11
#define MIX1 R12
#define PARTIAL_PTR R13
#define TMP R14
#define ZERO R15

	// Prepare a zero register.
	XORQ     ZERO, ZERO

	// Prepare Mixer.
	MOVQ     SEED, MIX0
	SUBQ     TOTAL_LEN, MIX0
	MOVQ     SEED, MIX1
	ADDQ     TOTAL_LEN, MIX1
	INCQ     MIX1
	MOVQ     MIX0, 0(SP)
	MOVQ     MIX1, 8(SP)
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X0, X0
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X1, X1
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X2, X2
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X3, X3
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X4, X4
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X5, X5
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X6, X6
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X7, X7
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X8, X8
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X9, X9
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X10, X10
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X11, X11
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X12, X12
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X13, X13
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X14, X14
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN

	// Handle final sub 16-byte block.

sub16:
	CMPQ     SRC_LEN, $0
	JE       combine
	MOVQ     ZERO, 16(SP)
	MOVQ     ZERO, 24(SP)
	LEAQ     16(SP), PARTIAL_PTR
	CMPQ     TOTAL_LEN, $16
	JB       byteloop
	LEAQ     -16(SRC_PTR)(SRC_LEN*1), SRC_PTR
	MOVQ     $16, SRC_LEN

byteloop:
	MOVB     (SRC_PTR), TMP
	MOVB     TMP, (PARTIAL_PTR)
	INCQ     SRC_PTR
	INCQ     PARTIAL_PTR
	DECQ     SRC_LEN
	JNE      byteloop
	VAESDEC  16(SP), X15, X15

	// Combine.

combine:
	VAESDEC  X10, X7, X7
	VAESDEC  X4, X7, X7
	VAESDEC  X5, X7, X7
	VAESDEC  X12, X7, X7
	VAESDEC  X8, X7, X7
	VAESDEC  X0, X7, X7
	VAESDEC  X1, X7, X7
	VAESDEC  X9, X7, X7
	VAESDEC  X13, X7, X7
	VAESDEC  X2, X7, X7
	VAESDEC  X6, X7, X7
	VAESDEC  X14, X7, X7
	VAESDEC  X3, X7, X7
	VAESDEC  X11, X7, X7
	VAESDEC  X15, X7, X7

	// Mixing.
	VAESDEC  0(SP), X7, X7
	VAESDEC  0(SP), X7, X7
	VAESDEC  0(SP), X7, X7

	// Store hash.
	MOVOU    X7, 0(DST_PTR)
	RET
#undef SEED
#undef DST_PTR
#undef SRC_PTR
#undef SRC_LEN
#undef TOTAL_LEN
#undef MIX0
#undef MIX1
#undef PARTIAL_PTR
#undef TMP
#undef ZERO

TEXT ·blocks128(SB),0,$0-48
#define S_PTR DI
	MOVQ     s_ptr+0(FP), S_PTR
#define SRC_PTR SI
	MOVQ     src_ptr+24(FP), SRC_PTR
#define SRC_LEN AX
	MOVQ     src_len+32(FP), SRC_LEN
	MOVOU    0(S_PTR), X0
	MOVOU    16(S_PTR), X1
	MOVOU    32(S_PTR), X2
	MOVOU    48(S_PTR), X3
	MOVOU    64(S_PTR), X4
	MOVOU    80(S_PTR), X5
	MOVOU    96(S_PTR), X6
	MOVOU    112(S_PTR), X7
	MOVOU    128(S_PTR), X8
	MOVOU    144(S_PTR), X9
	MOVOU    160(S_PTR), X10
	MOVOU    176(S_PTR), X11
	MOVOU    192(S_PTR), X12
	MOVOU    208(S_PTR), X13
	MOVOU    224(S_PTR), X14
	MOVOU    240(S_PTR), X15

loop:
	CMPQ     SRC_LEN, $256
	JB       done
	VAESDEC  0(SRC_PTR), X0, X0
	VAESDEC  16(SRC_PTR), X1, X1
	VAESDEC  32(SRC_PTR), X2, X2
	VAESDEC  48(SRC_PTR), X3, X3
	VAESDEC  64(SRC_PTR), X4, X4
	VAESDEC  80(SRC_PTR), X5, X5
	VAESDEC  96(SRC_PTR), X6, X6
	VAESDEC  112(SRC_PTR), X7, X7
	VAESDEC  128(SRC_PTR), X8, X8
	VAESDEC  144(SRC_PTR), X9, X9
	VAESDEC  160(SRC_PTR), X10, X10
	VAESDEC  176(SRC_PTR), X11, X11
	VAESDEC  192(SRC_PTR), X12, X12
	VAESDEC  208(SRC_PTR), X13, X13
	VAESDEC  224(SRC_PTR), X14, X14
	VAESDEC  240(SRC_PTR), X15, X15

	// Update source pointer.
	ADDQ     $256, SRC_PTR
	SUBQ     $256, SRC_LEN
	JMP      loop

done:
	MOVOU    X0, 0(S_PTR)
	MOVOU    X1, 16(S_PTR)
	MOVOU    X2, 32(S_PTR)
	MOVOU    X3, 48(S_PTR)
	MOVOU    X4, 64(S_PTR)
	MOVOU    X5, 80(S_PTR)
	MOVOU    X6, 96(S_PTR)
	MOVOU    X7, 112(S_PTR)
	MOVOU    X8, 128(S_PTR)
	MOVOU    X9, 144(S_PTR)
	MOVOU    X10, 160(S_PTR)
	MOVOU    X11, 176(S_PTR)
	MOVOU    X12, 192(S_PTR)
	MOVOU    X13, 208(S_PTR)
	MOVOU    X14, 224(S_PTR)
	MOVOU    X15, 240(S_PTR)
	RET
#undef S_PTR
#undef SRC_PTR
#undef SRC_LEN

TEXT ·checksum256(SB),0,$32-56
#define SEED R8
	MOVQ     seed+0(FP), SEED
#define DST_PTR DI
	MOVQ     dst_ptr+8(FP), DST_PTR
#define SRC_PTR SI
	MOVQ     src_ptr+32(FP), SRC_PTR
#define SRC_LEN AX
	MOVQ     src_len+40(FP), SRC_LEN

	// Backup total input length.
#define TOTAL_LEN R9
	MOVQ     SRC_LEN, TOTAL_LEN

	// Load zero "IV".
	VPXORQ   Y16, Y16, Y16
	VPXORQ   Y17, Y17, Y17
	VPXORQ   Y18, Y18, Y18
	VPXORQ   Y19, Y19, Y19
	VPXORQ   Y20, Y20, Y20
	VPXORQ   Y21, Y21, Y21
	VPXORQ   Y22, Y22, Y22
	VPXORQ   Y23, Y23, Y23

	// Handle full 256-byte blocks.

loop:
	CMPQ     SRC_LEN, $256
	JB       sub256

	// Hash block.
	VAESDEC  0(SRC_PTR), Y16, Y16
	VAESDEC  32(SRC_PTR), Y17, Y17
	VAESDEC  64(SRC_PTR), Y18, Y18
	VAESDEC  96(SRC_PTR), Y19, Y19
	VAESDEC  128(SRC_PTR), Y20, Y20
	VAESDEC  160(SRC_PTR), Y21, Y21
	VAESDEC  192(SRC_PTR), Y22, Y22
	VAESDEC  224(SRC_PTR), Y23, Y23

	// Update source pointer.
	ADDQ     $256, SRC_PTR
	SUBQ     $256, SRC_LEN
	JMP      loop

	// Handle final sub 256-byte block.

sub256:
	VEXTRACTI32X4 $0, Y16, X0
	VEXTRACTI32X4 $1, Y16, X1
	VEXTRACTI32X4 $0, Y17, X2
	VEXTRACTI32X4 $1, Y17, X3
	VEXTRACTI32X4 $0, Y18, X4
	VEXTRACTI32X4 $1, Y18, X5
	VEXTRACTI32X4 $0, Y19, X6
	VEXTRACTI32X4 $1, Y19, X7
	VEXTRACTI32X4 $0, Y20, X8
	VEXTRACTI32X4 $1, Y20, X9
	VEXTRACTI32X4 $0, Y21, X10
	VEXTRACTI32X4 $1, Y21, X11
	VEXTRACTI32X4 $0, Y22, X12
	VEXTRACTI32X4 $1, Y22, X13
	VEXTRACTI32X4 $0, Y23, X14
	VEXTRACTI32X4 $1, Y23, X15

	// Allocate general purpose registers.
#define MIX0 R11
#define MIX1 R12
#define PARTIAL_PTR R13
#define TMP R14
#define ZERO R15

	// Prepare a zero register.
	XORQ     ZERO, ZERO

	// Prepare Mixer.
	MOVQ     SEED, MIX0
	SUBQ     TOTAL_LEN, MIX0
	MOVQ     SEED, MIX1
	ADDQ     TOTAL_LEN, MIX1
	INCQ     MIX1
	MOVQ     MIX0, 0(SP)
	MOVQ     MIX1, 8(SP)
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X0, X0
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X1, X1
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X2, X2
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X3, X3
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X4, X4
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X5, X5
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X6, X6
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X7, X7
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X8, X8
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X9, X9
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X10, X10
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X11, X11
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X12, X12
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X13, X13
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X14, X14
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN

	// Handle final sub 16-byte block.

sub16:
	CMPQ     SRC_LEN, $0
	JE       combine
	MOVQ     ZERO, 16(SP)
	MOVQ     ZERO, 24(SP)
	LEAQ     16(SP), PARTIAL_PTR
	CMPQ     TOTAL_LEN, $16
	JB       byteloop
	LEAQ     -16(SRC_PTR)(SRC_LEN*1), SRC_PTR
	MOVQ     $16, SRC_LEN

byteloop:
	MOVB     (SRC_PTR), TMP
	MOVB     TMP, (PARTIAL_PTR)
	INCQ     SRC_PTR
	INCQ     PARTIAL_PTR
	DECQ     SRC_LEN
	JNE      byteloop
	VAESDEC  16(SP), X15, X15

	// Combine.

combine:
	VAESDEC  X10, X7, X7
	VAESDEC  X4, X7, X7
	VAESDEC  X5, X7, X7
	VAESDEC  X12, X7, X7
	VAESDEC  X8, X7, X7
	VAESDEC  X0, X7, X7
	VAESDEC  X1, X7, X7
	VAESDEC  X9, X7, X7
	VAESDEC  X13, X7, X7
	VAESDEC  X2, X7, X7
	VAESDEC  X6, X7, X7
	VAESDEC  X14, X7, X7
	VAESDEC  X3, X7, X7
	VAESDEC  X11, X7, X7
	VAESDEC  X15, X7, X7

	// Mixing.
	VAESDEC  0(SP), X7, X7
	VAESDEC  0(SP), X7, X7
	VAESDEC  0(SP), X7, X7

	// Store hash.
	MOVOU    X7, 0(DST_PTR)
	RET
#undef SEED
#undef DST_PTR
#undef SRC_PTR
#undef SRC_LEN
#undef TOTAL_LEN
#undef MIX0
#undef MIX1
#undef PARTIAL_PTR
#undef TMP
#undef ZERO

TEXT ·blocks256(SB),0,$0-48
#define S_PTR DI
	MOVQ     s_ptr+0(FP), S_PTR
#define SRC_PTR SI
	MOVQ     src_ptr+24(FP), SRC_PTR
#define SRC_LEN AX
	MOVQ     src_len+32(FP), SRC_LEN
	VMOVDQU32 0(S_PTR), Y16
	VMOVDQU32 32(S_PTR), Y17
	VMOVDQU32 64(S_PTR), Y18
	VMOVDQU32 96(S_PTR), Y19
	VMOVDQU32 128(S_PTR), Y20
	VMOVDQU32 160(S_PTR), Y21
	VMOVDQU32 192(S_PTR), Y22
	VMOVDQU32 224(S_PTR), Y23

loop:
	CMPQ     SRC_LEN, $256
	JB       done
	VAESDEC  0(SRC_PTR), Y16, Y16
	VAESDEC  32(SRC_PTR), Y17, Y17
	VAESDEC  64(SRC_PTR), Y18, Y18
	VAESDEC  96(SRC_PTR), Y19, Y19
	VAESDEC  128(SRC_PTR), Y20, Y20
	VAESDEC  160(SRC_PTR), Y21, Y21
	VAESDEC  192(SRC_PTR), Y22, Y22
	VAESDEC  224(SRC_PTR), Y23, Y23

	// Update source pointer.
	ADDQ     $256, SRC_PTR
	SUBQ     $256, SRC_LEN
	JMP      loop

done:
	VMOVDQU32 Y16, 0(S_PTR)
	VMOVDQU32 Y17, 32(S_PTR)
	VMOVDQU32 Y18, 64(S_PTR)
	VMOVDQU32 Y19, 96(S_PTR)
	VMOVDQU32 Y20, 128(S_PTR)
	VMOVDQU32 Y21, 160(S_PTR)
	VMOVDQU32 Y22, 192(S_PTR)
	VMOVDQU32 Y23, 224(S_PTR)
	RET
#undef S_PTR
#undef SRC_PTR
#undef SRC_LEN

TEXT ·checksum512(SB),0,$32-56
#define SEED R8
	MOVQ     seed+0(FP), SEED
#define DST_PTR DI
	MOVQ     dst_ptr+8(FP), DST_PTR
#define SRC_PTR SI
	MOVQ     src_ptr+32(FP), SRC_PTR
#define SRC_LEN AX
	MOVQ     src_len+40(FP), SRC_LEN

	// Backup total input length.
#define TOTAL_LEN R9
	MOVQ     SRC_LEN, TOTAL_LEN

	// Load zero "IV".
	VPXORQ   Z16, Z16, Z16
	VPXORQ   Z17, Z17, Z17
	VPXORQ   Z18, Z18, Z18
	VPXORQ   Z19, Z19, Z19

	// Handle full 256-byte blocks.

loop:
	CMPQ     SRC_LEN, $256
	JB       sub256

	// Hash block.
	VAESDEC  0(SRC_PTR), Z16, Z16
	VAESDEC  64(SRC_PTR), Z17, Z17
	VAESDEC  128(SRC_PTR), Z18, Z18
	VAESDEC  192(SRC_PTR), Z19, Z19

	// Update source pointer.
	ADDQ     $256, SRC_PTR
	SUBQ     $256, SRC_LEN
	JMP      loop

	// Handle final sub 256-byte block.

sub256:
	VEXTRACTI32X4 $0, Z16, X0
	VEXTRACTI32X4 $1, Z16, X1
	VEXTRACTI32X4 $2, Z16, X2
	VEXTRACTI32X4 $3, Z16, X3
	VEXTRACTI32X4 $0, Z17, X4
	VEXTRACTI32X4 $1, Z17, X5
	VEXTRACTI32X4 $2, Z17, X6
	VEXTRACTI32X4 $3, Z17, X7
	VEXTRACTI32X4 $0, Z18, X8
	VEXTRACTI32X4 $1, Z18, X9
	VEXTRACTI32X4 $2, Z18, X10
	VEXTRACTI32X4 $3, Z18, X11
	VEXTRACTI32X4 $0, Z19, X12
	VEXTRACTI32X4 $1, Z19, X13
	VEXTRACTI32X4 $2, Z19, X14
	VEXTRACTI32X4 $3, Z19, X15

	// Allocate general purpose registers.
#define MIX0 R11
#define MIX1 R12
#define PARTIAL_PTR R13
#define TMP R14
#define ZERO R15

	// Prepare a zero register.
	XORQ     ZERO, ZERO

	// Prepare Mixer.
	MOVQ     SEED, MIX0
	SUBQ     TOTAL_LEN, MIX0
	MOVQ     SEED, MIX1
	ADDQ     TOTAL_LEN, MIX1
	INCQ     MIX1
	MOVQ     MIX0, 0(SP)
	MOVQ     MIX1, 8(SP)
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X0, X0
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X1, X1
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X2, X2
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X3, X3
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X4, X4
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X5, X5
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X6, X6
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X7, X7
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X8, X8
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X9, X9
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X10, X10
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X11, X11
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X12, X12
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X13, X13
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X14, X14
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN

	// Handle final sub 16-byte block.

sub16:
	CMPQ     SRC_LEN, $0
	JE       combine
	MOVQ     ZERO, 16(SP)
	MOVQ     ZERO, 24(SP)
	LEAQ     16(SP), PARTIAL_PTR
	CMPQ     TOTAL_LEN, $16
	JB       byteloop
	LEAQ     -16(SRC_PTR)(SRC_LEN*1), SRC_PTR
	MOVQ     $16, SRC_LEN

byteloop:
	MOVB     (SRC_PTR), TMP
	MOVB     TMP, (PARTIAL_PTR)
	INCQ     SRC_PTR
	INCQ     PARTIAL_PTR
	DECQ     SRC_LEN
	JNE      byteloop
	VAESDEC  16(SP), X15, X15

	// Combine.

combine:
	VAESDEC  X10, X7, X7
	VAESDEC  X4, X7, X7
	VAESDEC  X5, X7, X7
	VAESDEC  X12, X7, X7
	VAESDEC  X8, X7, X7
	VAESDEC  X0, X7, X7
	VAESDEC  X1, X7, X7
	VAESDEC  X9, X7, X7
	VAESDEC  X13, X7, X7
	VAESDEC  X2, X7, X7
	VAESDEC  X6, X7, X7
	VAESDEC  X14, X7, X7
	VAESDEC  X3, X7, X7
	VAESDEC  X11, X7, X7
	VAESDEC  X15, X7, X7

	// Mixing.
	VAESDEC  0(SP), X7, X7
	VAESDEC  0(SP), X7, X7
	VAESDEC  0(SP), X7, X7

	// Store hash.
	MOVOU    X7, 0(DST_PTR)
	RET
#undef SEED
#undef DST_PTR
#undef SRC_PTR
#undef SRC_LEN
#undef TOTAL_LEN
#undef MIX0
#undef MIX1
#undef PARTIAL_PTR
#undef TMP
#undef ZERO

TEXT ·blocks512(SB),0,$0-48
#define S_PTR DI
	MOVQ     s_ptr+0(FP), S_PTR
#define SRC_PTR SI
	MOVQ     src_ptr+24(FP), SRC_PTR
#define SRC_LEN AX
	MOVQ     src_len+32(FP), SRC_LEN
	VMOVDQU64 0(S_PTR), Z16
	VMOVDQU64 64(S_PTR), Z17
	VMOVDQU64 128(S_PTR), Z18
	VMOVDQU64 192(S_PTR), Z19

loop:
	CMPQ     SRC_LEN, $256
	JB       done
	VAESDEC  0(SRC_PTR), Z16, Z16
	VAESDEC  64(SRC_PTR), Z17, Z17
	VAESDEC  128(SRC_PTR), Z18, Z18
	VAESDEC  192(SRC_PTR), Z19, Z19

	// Update source pointer.
	ADDQ     $256, SRC_PTR
	SUBQ     $256, SRC_LEN
	JMP      loop

done:
	VMOVDQU64 Z16, 0(S_PTR)
	VMOVDQU64 Z17, 64(S_PTR)
	VMOVDQU64 Z18, 128(S_PTR)
	VMOVDQU64 Z19, 192(S_PTR)
	RET
#undef S_PTR
#undef SRC_PTR
#undef SRC_LEN

TEXT ·finish128(SB),0,$32-112
#define SEED R8
	MOVQ     seed+0(FP), SEED
#define S_PTR R9
	MOVQ     s_ptr+8(FP), S_PTR
#define DST_PTR DI
	MOVQ     dst_ptr+32(FP), DST_PTR
#define SRC_PTR SI
	MOVQ     src_ptr+56(FP), SRC_PTR
#define SRC_LEN AX
	MOVQ     src_len+64(FP), SRC_LEN
#define TRAIL_PTR R10
	MOVQ     trail_ptr+80(FP), TRAIL_PTR
#define TOTAL_LEN BX
	MOVQ     total_len+104(FP), TOTAL_LEN
	MOVOU    0(S_PTR), X0
	MOVOU    16(S_PTR), X1
	MOVOU    32(S_PTR), X2
	MOVOU    48(S_PTR), X3
	MOVOU    64(S_PTR), X4
	MOVOU    80(S_PTR), X5
	MOVOU    96(S_PTR), X6
	MOVOU    112(S_PTR), X7
	MOVOU    128(S_PTR), X8
	MOVOU    144(S_PTR), X9
	MOVOU    160(S_PTR), X10
	MOVOU    176(S_PTR), X11
	MOVOU    192(S_PTR), X12
	MOVOU    208(S_PTR), X13
	MOVOU    224(S_PTR), X14
	MOVOU    240(S_PTR), X15

	// Allocate general purpose registers.
#define MIX0 R11
#define MIX1 R12
#define PARTIAL_PTR R13
#define TMP R14
#define ZERO R15

	// Prepare a zero register.
	XORQ     ZERO, ZERO

	// Prepare Mixer.
	MOVQ     SEED, MIX0
	SUBQ     TOTAL_LEN, MIX0
	MOVQ     SEED, MIX1
	ADDQ     TOTAL_LEN, MIX1
	INCQ     MIX1
	MOVQ     MIX0, 0(SP)
	MOVQ     MIX1, 8(SP)
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X0, X0
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X1, X1
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X2, X2
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X3, X3
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X4, X4
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X5, X5
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X6, X6
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X7, X7
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X8, X8
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X9, X9
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X10, X10
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X11, X11
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X12, X12
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X13, X13
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN
	CMPQ     SRC_LEN, $16
	JB       sub16
	VAESDEC  0(SRC_PTR), X14, X14
	ADDQ     $16, SRC_PTR
	SUBQ     $16, SRC_LEN

	// Handle final sub 16-byte block.

sub16:
	CMPQ     SRC_LEN, $0
	JE       combine
	MOVQ     ZERO, 16(SP)
	MOVQ     ZERO, 24(SP)
	LEAQ     16(SP), PARTIAL_PTR
	CMPQ     TOTAL_LEN, $16
	JB       byteloop
	LEAQ     (TRAIL_PTR), SRC_PTR
	MOVQ     $16, SRC_LEN

byteloop:
	MOVB     (SRC_PTR), TMP
	MOVB     TMP, (PARTIAL_PTR)
	INCQ     SRC_PTR
	INCQ     PARTIAL_PTR
	DECQ     SRC_LEN
	JNE      byteloop
	VAESDEC  16(SP), X15, X15

	// Combine.

combine:
	VAESDEC  X10, X7, X7
	VAESDEC  X4, X7, X7
	VAESDEC  X5, X7, X7
	VAESDEC  X12, X7, X7
	VAESDEC  X8, X7, X7
	VAESDEC  X0, X7, X7
	VAESDEC  X1, X7, X7
	VAESDEC  X9, X7, X7
	VAESDEC  X13, X7, X7
	VAESDEC  X2, X7, X7
	VAESDEC  X6, X7, X7
	VAESDEC  X14, X7, X7
	VAESDEC  X3, X7, X7
	VAESDEC  X11, X7, X7
	VAESDEC  X15, X7, X7

	// Mixing.
	VAESDEC  0(SP), X7, X7
	VAESDEC  0(SP), X7, X7
	VAESDEC  0(SP), X7, X7

	// Store hash.
	MOVOU    X7, 0(DST_PTR)
	RET
#undef SEED
#undef S_PTR
#undef DST_PTR
#undef SRC_PTR
#undef SRC_LEN
#undef TRAIL_PTR
#undef TOTAL_LEN
#undef MIX0
#undef MIX1
#undef PARTIAL_PTR
#undef TMP
#undef ZERO
